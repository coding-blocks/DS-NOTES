
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Linear Regression &#8212; Data Science Notes</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Multi Variable Regression" href="3.2%20Multi-Variate%20Regression.html" />
    <link rel="prev" title="K - Nearest Neighbour" href="2.%20KNN.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data Science Notes</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Introduction
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="1.1%20Introduction%20to%20Numpy.html">
   Numpy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="1.2%20Introduction%20to%20Matplotlib.html">
   Matplotlib: Visualization with Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="1.3%20Introduction%20to%20Pandas.html">
   Pandas
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="2.%20KNN.html">
   K - Nearest Neighbour
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3.2%20Multi-Variate%20Regression.html">
   Multi Variable Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3.3%20MLE%20-%20Linear%20Regression.html">
   MLE - Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3.4%20GLM%20-%20Linear%20Regression.html">
   Generalised linear model-Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="4.%20Gradient%20Descent.html">
   Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5.1%20%20Logistic%20Regression.html">
   Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="5.2%20Maximum%20Likelihood%20Estimation%20and%20Implementation.html">
   Logistic Regression MLE &amp; Implementation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="6.%20Decision%20Trees.html">
   Decision Tree Algorithm
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="7.%20Ensemble.html">
   Ensemble Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="9.1%20Naive%20Bayes.html">
   Naive Bayes Algorithm
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="9.2%20Multinomial%20Naive%20Bayes.html">
   Multinomial Naive Bayes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11.%20Imbalanced%20Dataset.html">
   Imbalanced Dataset
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="12.%20PCA.html">
   Principal Component Analysis
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  About
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="About%20the%20Authors.html">
   About the Authors
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/3.1 Linear Regression.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/3.1 Linear Regression.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simple-linear-regression">
   Simple Linear Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#objective">
     Objective
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loss-function-or-cost-function">
     Loss Function Or Cost Function
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#squared-error">
       Squared Error
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#proof-for-univariate-linear-regression">
   Proof For Univariate Linear Regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#first-calculating-for-c">
     First calculating for c:-
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#calculating-for-m">
     Calculating for m:-
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#putting-optimal-value-of-c-derived-above-in-the-proof">
       Putting optimal value of c derived above in the proof:-
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conclusion">
     Conclusion
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#code">
   Code
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#r2-score">
   R2 Score
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#formula-for-calculating-r2-score">
     Formula for calculating r2 score
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#code-for-r2-score">
     Code For R2 Score
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#assumptions-of-linear-regression">
   Assumptions of Linear Regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overfitting-and-underfitting">
   Overfitting and Underfitting
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#overfitting">
     Overfitting
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#underfitting">
     Underfitting
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-reading">
   Further Reading
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="linear-regression">
<h1>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">¶</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Linear Regression is a machine learning algorithm based on supervised learning. It performs a regression task i.e to predict a continous value. Linear regression attempts to model the relationship between two variables by <strong>fitting a linear equation to observed data</strong>. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable. For example, a modeler might want to relate the weights of individuals to their heights using a linear regression model.</p>
<p>Regression models a value based target prediction on independent variables. It is mostly used for finding out the relationship between variables and forecasting. Different regression models differ based on – the kind of relationship between dependent and independent variables, they are considering and the number of independent variables being used.</p>
</section>
<section id="simple-linear-regression">
<h2>Simple Linear Regression<a class="headerlink" href="#simple-linear-regression" title="Permalink to this headline">¶</a></h2>
<p>In This Section we will discuss <strong>Simple Linear Regression</strong>.
Simple linear regression is a type of regression analysis where the number of independent variables is one and there is a linear relationship between the independent(x) and dependent(y) variable.</p>
<p>Linear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x). So, this regression technique finds out a <strong>linear relationship</strong> between x (input) and y(output) and variation in predicted value from actual answer is due to random noise . Hence, the name is Linear Regression.</p>
<p>Simple linear regression is used to find out the best relationship between a single input variable (predictor, independent variable, input feature, input parameter) &amp; output variable (predicted, dependent variable, output feature, output parameter) provided that both variables are continuous in nature. This relationship represents how an input variable is related to the output variable and how it is represented by a straight line.</p>
<p>To understand this concept, let us have a look at scatter plots. Scatter diagrams or plots provides a graphical representation of the relationship of two continuous variables.</p>
<p><img alt="" src="_images/linear_regression11.jpg" /></p>
<section id="objective">
<h3>Objective<a class="headerlink" href="#objective" title="Permalink to this headline">¶</a></h3>
<p>In The above Diagram X(Independent variable) is plotted Against Y(Dependent variable) Blue points Represent The Data.
The red line in the above graph is referred to as the best fit straight line. Based on the given data points, we try to plot a line that models the points the best. The line can be modelled based on the linear equation shown below.</p>
<p><span class="math notranslate nohighlight">\(Y = m\times X + c\)</span> is the general equation of univariate linear relation between variables. Where-</p>
<p><em>m</em> is the slope of line,</p>
<p>X is the input data,</p>
<p>c is y intercept, also called bias</p>
<p><strong>The motive of the  simple linear regression algorithm is to find the best values for m and c and then use to predict values</strong>.
The line having that value of m and c which is used for prediction is called <strong>Best Fit Line</strong>  For above given example red line is the best fit line.</p>
</section>
<section id="loss-function-or-cost-function">
<h3>Loss Function Or Cost Function<a class="headerlink" href="#loss-function-or-cost-function" title="Permalink to this headline">¶</a></h3>
<p>To measure how close our model’s prediction is to the actual values we have to select a method to measure it for every prediction we make. Choosing this is very important as we will improve our parameters by minimising the loss given by loss function.</p>
<p><img alt="reg_front.svg" src="_images/linear_regression12.png" /></p>
<p>Taking diffrence in the value of all predicted points to it’s actual value and then sqauring it up and then adding all values will give us the total error in the prediction.</p>
<p>Squaring prevents the negative value to decrease the total error when addition will be performed</p>
<p>This function is called <em>Cost Function</em>  Or <em>Loss Function</em>
There can be different types of cost function above discussed cost function <strong>Squared Error</strong> Dividing The value by total no. of data points will result in <strong>Mean Squared Error</strong></p>
<section id="squared-error">
<h4>Squared Error<a class="headerlink" href="#squared-error" title="Permalink to this headline">¶</a></h4>
<blockquote>
<div><p><strong>Loss</strong> = <span class="math notranslate nohighlight">\(\sum_{i=1}^n (Y_{actual}{i}-Y_{pred}{i})^2\)</span></p>
</div></blockquote>
<p><em>We have to minimise the loss</em> thus minimise the above written expression. Looking Closely the expression <strong>L</strong> (Loss) is function of two values <span class="math notranslate nohighlight">\(Y_{actual}\)</span> and <span class="math notranslate nohighlight">\(Y_{pred}\)</span> which is the actual <strong>given answer</strong> of the data and the <strong>predicted value</strong> by our machine learning model respectively. As Discussed Above in case of univariate linear regression predicted value of y will be calculated from equation of line i.e.</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(Y_{pred} = m \times X+ c \hspace{1cm} \begin{cases}
      \text{ m =&gt; Slope} \\
      \text{ c =&gt; y-intercept}
    \end{cases}\)</span></p>
</div></blockquote>
<p>So the Loss Function becomes:</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(Loss = \sum_{i=1}^n (Y_{actual}{i}-m*X_{i}-c)^2\)</span></p>
<p><strong>L</strong> (Loss) is function of three values <span class="math notranslate nohighlight">\(Y_{actual}\)</span> ,m and c</p>
</div></blockquote>
<p>As <span class="math notranslate nohighlight">\(Y_{actual}\)</span> are constant and is fixed for a single case
<strong>L</strong> (Loss) is dependent on two values m and c. Objective is to find value of m and c such that loss is minimum. <strong>Minimizing Loss here means minimizing the total error of the model.</strong></p>
</section>
</section>
</section>
<section id="proof-for-univariate-linear-regression">
<h2>Proof For Univariate Linear Regression<a class="headerlink" href="#proof-for-univariate-linear-regression" title="Permalink to this headline">¶</a></h2>
<p>The below proof uses application of derivatives (Minima) concept you can refer to mentioned video for quick revision  or your high school notes for more indepth intiution
<a class="reference external" href="https://www.youtube.com/watch?v=pInFesXIfg8">https://www.youtube.com/watch?v=pInFesXIfg8</a></p>
<p>Loss function :-</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(L = \sum_{i=1}^n (Y_{actual}{i}-m*X_{i}-c)^2\)</span></p>
</div></blockquote>
<p>where, <strong>L</strong> is Loss</p>
<p><strong>The optimal value of parameters(in this case m and c) occurs when loss(l) is minimum</strong></p>
<blockquote>
<div><p>To find the value of <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(c\)</span> for which <span class="math notranslate nohighlight">\(L\)</span> is minimum, for that we will calculate derivative of <span class="math notranslate nohighlight">\(L\)</span>, <em>with respect to</em> <span class="math notranslate nohighlight">\(m\)</span> and then <span class="math notranslate nohighlight">\(c\)</span> and then put is equal to 0 to find the critical value of <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(c\)</span> where <span class="math notranslate nohighlight">\(L\)</span> is minimum.</p>
</div></blockquote>
<p>As there are two independent variables(m,c) We have to calculate <em>partial derivative</em> and then calculate minima. You can read more about partial diffrentiation here-</p>
<p><a class="reference external" href="https://cutt.ly/OQSFa8t">https://cutt.ly/OQSFa8t</a></p>
<p><span class="math notranslate nohighlight">\(L = \sum_{i=1}^n (Y_{actual}{i}-m \times X_{i}-c)^2\)</span></p>
<section id="first-calculating-for-c">
<h3>First calculating for c:-<a class="headerlink" href="#first-calculating-for-c" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial c}=0\)</span></p>
<p><span class="math notranslate nohighlight">\(\dfrac{\partial( {\sum_{i=1}^n (Y_{actual}{i}-m \times X_{i}-c)^2})}{\partial c}=0\)</span></p>
<p><span class="math notranslate nohighlight">\(\sum_{i=1}^n{\dfrac{\partial(Y_{actual}{i}-m \times X_{i}-c)^2}{\partial c}}=0\)</span></p>
</div></blockquote>
<p>Applying Chain Rule:-</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\sum_{i=1}^n 2 \times (Y_{actual}{i}-m \times X_{i}-c) \times (-1)=0\)</span></p>
<p><span class="math notranslate nohighlight">\(\sum_{i=1}^n (Y_{actual}{i}-m \times X_{i}-c)=0\)</span></p>
</div></blockquote>
<p>Closely observing above equation you will find that it is the sum of all <span class="math notranslate nohighlight">\(Y_{actual} - Y_{pred}\)</span> which is also called <strong>sum of residuals.</strong></p>
<p>Thus, Above eqution proves that for the best fit line <strong>sum of all residuals is zero</strong></p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\sum_{i=1}^n Y_{actual}{i} - \sum_{i=1}^n m \times X_{i} - \sum_{i=1}^n c = 0\)</span></p>
</div></blockquote>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\sum_{i=1}^n Y_{actual}{i} - m \times \sum_{i=1}^n X_{i} - n \times c = 0\)</span></p>
</div></blockquote>
<p><em>Dividing The Whole Equation By n:-</em></p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\dfrac{1}{n} \times \sum_{i=1}^n Y_{actual}{i} - m \times \dfrac{1}{n} \times \sum_{i=1}^n X_{i} - c =0 \hspace{1.5cm}\)</span>   <strong>–&gt; Equation 1</strong></p>
</div></blockquote>
<blockquote>
<div><p>We can write this equation as:</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\dfrac{1}{n} \times \sum_{i=1}^n Y_{actual}{i} = Y_{mean} \hspace{1cm}\)</span>   —- <strong>equation A</strong></p>
<p><span class="math notranslate nohighlight">\(\dfrac{1}{n} \times \sum_{i=1}^n X_{i} = X_{mean} \hspace{1.5cm}\)</span>  —- <strong>equation B</strong></p>
</div></blockquote>
</div></blockquote>
<p><em>Putting values From A and B in equation 1:-</em></p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(Y_{mean} - m* X_{mean} - c =0 \)</span></p>
<blockquote>
<div><blockquote>
<div><p><span class="math notranslate nohighlight">\(\therefore c=  Y_{mean} - m\times X_{mean}\)</span></p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
<p><strong>Hence we got the optimal value for c in terms of mean of y and x and m. Every value in above expression can be calculated with the help of given data, as further in the proof we are gonna find the optimal value of m</strong></p>
</section>
<section id="calculating-for-m">
<h3>Calculating for m:-<a class="headerlink" href="#calculating-for-m" title="Permalink to this headline">¶</a></h3>
<p><span class="math notranslate nohighlight">\(\dfrac{\partial L}{\partial m}=0\)</span></p>
<p><span class="math notranslate nohighlight">\(\dfrac{\partial( {\sum_{i=1}^n (Y_{actual}{i}-m \times X_{i}-c)^2})}{\partial m}=0\)</span></p>
<p><span class="math notranslate nohighlight">\(\sum_{i=1}^n{\dfrac{\partial(Y_{actual}{i}-m \times X_{i}-c)^2}{\partial m}}=0\)</span></p>
<blockquote>
<div><p>Applying Chain rule:-</p>
</div></blockquote>
<p><span class="math notranslate nohighlight">\(\sum_{i=1}^n 2 \times (Y_{actual}{i}-m \times X_{i}-c) \times (-X_{i})=0\)</span></p>
<p><span class="math notranslate nohighlight">\(\sum_{i=1}^n (Y_{actual}{i}-m \times X_{i}-c) \times (X_{i})=0\)</span></p>
<p><span class="math notranslate nohighlight">\(\sum_{i=1}^n (Y_{actual}{i} \times X_{i}-m \times X_{i}^2-c \times X_{i})=0\)</span></p>
<section id="putting-optimal-value-of-c-derived-above-in-the-proof">
<h4>Putting optimal value of c derived above in the proof:-<a class="headerlink" href="#putting-optimal-value-of-c-derived-above-in-the-proof" title="Permalink to this headline">¶</a></h4>
<p><span class="math notranslate nohighlight">\(\sum_{i=1}^n (Y_{actual}{i} \times X_{i}-m \times X_{i}^2-(Y_{mean}-m \times X_{mean}) \times X_{i})=0\)</span></p>
<p><span class="math notranslate nohighlight">\(\sum_{i=1}^n (Y_{actual}{i} \times X_{i}-m \times X_{i}^2-Y_{mean} \times X_{i}+m \times X_{mean} \times X_{i})=0\)</span></p>
<p><span class="math notranslate nohighlight">\(\sum_{i=1}^n Y_{actual}{i} \times X_{i}-\sum_{i=1}^n m \times X_{i}^2-\sum_{i=1}^n Y_{mean} \times X_{i}+\sum_{i=1}^n m \times X_{mean} \times X_{i}=0\)</span></p>
<p><span class="math notranslate nohighlight">\(\small{\sum_{i=1}^n Y_{actual}{i}*X_{i}-m*\sum_{i=1}^n X_{i}^2-Y_{mean}*\sum_{i=1}^n X_{i}+m*X_{mean}*\sum_{i=1}^n X_{i}=0}\)</span></p>
<blockquote>
<div><p>Dividing whole Expression By n :-</p>
</div></blockquote>
<p><span class="math notranslate nohighlight">\(\dfrac{1}{n}  \times  \sum_{i=1}^n Y_{actual}{i} \times X_{i}-m \times \dfrac{1}{n}\sum_{i=1}^n X_{i}^2-Y_{mean} \times \dfrac{1}{n}  \times \sum_{i=1}^n X_{i}+m*X_{mean} \times \dfrac{1}{n}  \times \sum_{i=1}^n X_{i}=0\)</span></p>
<blockquote>
<div><p>Looking closely to the above written expression we can conclude that</p>
<p><span class="math notranslate nohighlight">\(\dfrac{1}{n}  \times  \sum_{i=1}^n Y_{actual}{i} \times X_{i}= (X \times Y)_{mean}\)</span>   —–<strong>equation C</strong></p>
<p><span class="math notranslate nohighlight">\(\dfrac{1}{n}\sum_{i=1}^n X_{i}^2 = X^2_{mean}\)</span>   ——<strong>equation D</strong></p>
</div></blockquote>
<hr class="docutils" />
<p><strong>Using equation A,B,C and D</strong> :-</p>
<p><span class="math notranslate nohighlight">\((X \times Y)_{mean}-m \times X^2_{mean}-Y_{mean} \times X_{mean}+ m \times X_{mean} \times X_{mean} =0\)</span></p>
<p><span class="math notranslate nohighlight">\((X \times Y)_{mean}-m \times X^2_{mean}-Y_{mean} \times X_{mean}+ m \times (X_{mean})^2 =0\)</span></p>
<blockquote>
<div><p><em>Please note the diffrence between</em> <span class="math notranslate nohighlight">\(X^2_{mean}\)</span> <em>and</em> <span class="math notranslate nohighlight">\((X_{mean})^2\)</span></p>
</div></blockquote>
<p><span class="math notranslate nohighlight">\(-m \times (X^2_{mean}-(X_{mean})^2) +(X \times Y)_{mean}-Y_{mean} \times X_{mean}=0\)</span></p>
<p><span class="math notranslate nohighlight">\(-m \times (X^2_{mean}-(X_{mean})^2) =-((X \times Y)_{mean}-Y_{mean} \times X_{mean})=0\)</span></p>
<p><span class="math notranslate nohighlight">\(m \times (X^2_{mean}-(X_{mean})^2) = ((X \times Y)_{mean}-Y_{mean} \times X_{mean})=0\)</span></p>
<blockquote>
<div><blockquote>
<div><blockquote>
<div><p><span class="math notranslate nohighlight">\(\therefore m= \dfrac{(X \times Y)_{mean}-Y_{mean} \times X_{mean}}{X^2_{mean}-(X_{mean})^2}\)</span></p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
<p><strong>Hence we got the optimal value for m in terms of mean of y, x, x^2 ans y*x. Every value in above expression can be calculated with the help of given data</strong></p>
</section>
</section>
<section id="conclusion">
<h3>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h3>
<p>For the best fit line in case univariate simple linear regression value of parameter m and c are-</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(m= \dfrac{(X \times Y)_{mean}-Y_{mean} \times X_{mean}}{X^2_{mean}-(X_{mean})^2}\)</span></p>
</div></blockquote>
<p>and</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(c=  Y_{mean} - m \times X_{mean}\)</span></p>
</div></blockquote>
<p>One important fact is that equation for parameters are not universal or fixed and is dependent on the loss function we choose in this case the formula are so because we used <strong>Squared error</strong> any other loss method may result in diffrent formula but <strong>Optimal value of parameters for linear regression is fixed</strong></p>
<p>Thus if chosen correct loss function and minimisation technique final value of m and c will be fixed for a given set of data</p>
</section>
</section>
<section id="code">
<h2>Code<a class="headerlink" href="#code" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<p>Below is a <code class="docutils literal notranslate"><span class="pre">House</span> <span class="pre">price</span> <span class="pre">dataset</span></code>, having only single feature of <strong>land size</strong>. So X contains land size(in sq. feet) of some houses of a locality, and y contains their corresponding <strong>price</strong> (in crores).</p>
<p><strong>X looks like :</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="p">[[</span><span class="mf">1599.60448522</span><span class="p">],</span>  
    <span class="p">[</span><span class="mf">1793.63237666</span><span class="p">],</span>  
    <span class="p">[</span><span class="mf">4588.03138463</span><span class="p">],</span>  
    <span class="p">[</span><span class="mf">3621.75315899</span><span class="p">],</span>  
    <span class="p">[</span><span class="mf">4343.08060035</span><span class="p">],</span><span class="o">...</span> 
    <span class="mi">50</span> <span class="n">instances</span><span class="p">]</span>  
</pre></div>
</div>
<p><strong>y looks like:</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="mf">2.65917039</span><span class="p">,</span> <span class="mf">2.27477643</span><span class="p">,</span> <span class="mf">5.08075256</span><span class="p">,</span> <span class="mf">3.97100962</span><span class="p">,</span> <span class="mf">4.61060872</span><span class="p">,</span><span class="o">...</span> <span class="mi">50</span> <span class="n">instances</span><span class="p">]</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># First 10 data points</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">10</span><span class="p">,:])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-------------------------&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">[:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1599.60448522]
 [1793.63237666]
 [4588.03138463]
 [3621.75315899]
 [4343.08060035]
 [1754.65388987]
 [2864.89328112]
 [5610.65542828]
 [5314.54707608]
 [4114.90954614]]
-------------------------
[2.65917039 2.27477643 5.08075256 3.97100962 4.61060872 2.36864491
 3.52940456 5.92555912 6.01264913 5.09355588]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Loading the data</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;Data/KNN/Regression/X_data.npy&quot;</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;Data/KNN/Regression/Y_data.npy&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(50, 1) (50,)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Viualizing data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Land Size (in sq. feet)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;House Price (in crores)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/3.1 Linear Regression_24_0.png" src="_images/3.1 Linear Regression_24_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SimpleLinearRegression</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1">#intializing the parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="o">=</span><span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c</span><span class="o">=</span><span class="mi">0</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="c1"># calculating required values</span>
        <span class="n">xy_mean</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">x_mean</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">y_mean</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">x_2_mean</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="c1"># calculating parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="p">(</span><span class="n">xy_mean</span> <span class="o">-</span> <span class="n">y_mean</span><span class="o">*</span><span class="n">x_mean</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">x_2_mean</span> <span class="o">-</span> <span class="p">(</span><span class="n">x_mean</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c</span> <span class="o">=</span> <span class="n">y_mean</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="o">*</span><span class="n">x_mean</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">y_pred</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">c</span>
        <span class="k">return</span> <span class="n">y_pred</span>

    <span class="k">def</span> <span class="nf">r2_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">y_true</span><span class="p">,</span><span class="n">y_pred</span><span class="p">):</span>
        <span class="n">y_mean</span><span class="o">=</span><span class="n">y_true</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">ratio</span><span class="o">=</span><span class="p">((</span><span class="n">y_true</span><span class="o">-</span><span class="n">y_pred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">((</span><span class="n">y_true</span><span class="o">-</span><span class="n">y_mean</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">r2_score</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="n">ratio</span>
        <span class="k">return</span> <span class="n">r2_score</span>
        
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Training of the model</span>
<span class="n">regressor</span><span class="o">=</span><span class="n">SimpleLinearRegression</span><span class="p">()</span>
<span class="n">regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Making Predictions</span>
<span class="n">y_pred</span><span class="o">=</span><span class="n">regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(50, 1)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visulaizing best fit line</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y_pred</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Land Size (in sq. feet)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;House Price (in crores)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/3.1 Linear Regression_28_0.png" src="_images/3.1 Linear Regression_28_0.png" />
</div>
</div>
<p><strong>The red line shown above is the best fit line and is used for predictions</strong></p>
</section>
<section id="r2-score">
<h2>R2 Score<a class="headerlink" href="#r2-score" title="Permalink to this headline">¶</a></h2>
<p>We use given data to calculate the parameters and then form the best fit line this process is called <code class="docutils literal notranslate"><span class="pre">Training</span></code> of the model now we can use the trained model to predict further unseen data this process is called <code class="docutils literal notranslate"><span class="pre">Testing</span></code></p>
<p>To measure how good our model is on testing data we use different methods one such method is calculating accuracy which is no. of correct prediction made But the most widely used measure is <strong>r2 score</strong></p>
<p>The idea behind r2 score is to compare the model with the mean model which is let’s assume you are given some data and asked to predict the answer for some unseen data your most safe bid will we to calculate the mean and give that as the answer this is the most basic method with which you can answer.</p>
<p>The whole motive of the Machine learning algorithm was to learn from data and then give the answer thus, our model should be compared from the most basic thing we can do to answer that same question which is the mean model</p>
<p>Generally, the Range of r2 score lies between <code class="docutils literal notranslate"><span class="pre">0-1</span></code> as if our model’s error is equal to mean model error value of r2 score will be zero if our model is so bad that it’s error is even greater than that of mean model value of r2 score will be negative and when our model error will be zero it will have the highest value of 1</p>
<p>Therefore, Higher The r2 score better the model</p>
<p><img alt="Regression-terminologies.png" src="_images/linear_regression1.png" /></p>
<section id="formula-for-calculating-r2-score">
<h3>Formula for calculating r2 score<a class="headerlink" href="#formula-for-calculating-r2-score" title="Permalink to this headline">¶</a></h3>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(R^2 = 1 - \dfrac{\sum_{i=1}^{n}(Y_{actual}{i}-Y_{pred}{i})^2}{\sum_{i=1}^{n}(Y_{actual}{i}-Y_{mean})^2}\)</span></p>
</div></blockquote>
</section>
<section id="code-for-r2-score">
<h3>Code For R2 Score<a class="headerlink" href="#code-for-r2-score" title="Permalink to this headline">¶</a></h3>
<p>For the coding part it can be easily calculated by loops and mathematical operation in python or my numpy operations(as shown above in the code), but for this i also want to introduce sklearn to you. sklearn is a very popular machine learning library and has many operation we will be discussing it in brief in further topics. it can be easily installed by running</p>
<p><em><code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">sklearn</span></code></em>
in your command prompt or terminal or</p>
<p><em><code class="docutils literal notranslate"><span class="pre">!pip</span> <span class="pre">install</span> <span class="pre">sklearn</span></code></em> in a your jupyter notebook code cell</p>
<p><code class="docutils literal notranslate"><span class="pre">r2_score</span></code> function takes two argument <strong>y_true</strong> which is a array containing true(actual) values and <strong>y_pred</strong> array of our predicted values by our model</p>
<p><code class="docutils literal notranslate"><span class="pre">r2_score(y_true,</span> <span class="pre">y_pred,</span> <span class="pre">*,</span> <span class="pre">sample_weight=None,</span> <span class="pre">multioutput='uniform_average')</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_actual</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span> <span class="c1">#importing the fuction</span>
<span class="n">score</span><span class="o">=</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_actual</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span> <span class="c1">#calling and passing arguments to the function</span>
<span class="nb">print</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.9373050951142091
</pre></div>
</div>
</div>
</div>
<p>Comparing with self-made r2 score function</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">score2</span><span class="o">=</span><span class="n">regressor</span><span class="o">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_actual</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">score2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.9373050951142091
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">Hence</span> <span class="pre">we</span> <span class="pre">have</span> <span class="pre">got</span> <span class="pre">r2</span> <span class="pre">score</span> <span class="pre">of</span> <span class="pre">about-</span></code> <em><code class="docutils literal notranslate"><span class="pre">0.937</span></code></em></p>
<p>You can read more about r2 score from here-</p>
<blockquote>
<div><p><a class="reference external" href="https://en.wikipedia.org/wiki/Coefficient_of_determination">https://en.wikipedia.org/wiki/Coefficient_of_determination</a></p>
</div></blockquote>
<p>and about sklearn coding’s part from here-</p>
<blockquote>
<div><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html</a></p>
</div></blockquote>
</section>
</section>
<section id="assumptions-of-linear-regression">
<h2>Assumptions of Linear Regression<a class="headerlink" href="#assumptions-of-linear-regression" title="Permalink to this headline">¶</a></h2>
<p>When we apply linear regression in general to any dataset we take assumption about that these assumptions help us to apply pre-defined set of rules and formula to determine paarmeters. if any of such assumption fails there will be a drop in both training and testing accuracy.</p>
<p><strong>There are four general assumptions associated with a linear regression model:</strong></p>
<ol class="simple">
<li><p><strong>Linearity:</strong> The relationship between X and the mean of Y is linear.</p></li>
<li><p><strong>Homoscedasticity:</strong> The variance of residual is the same for any value of X.</p></li>
<li><p><strong>Independence:</strong> Observations are independent of each other.</p></li>
<li><p><strong>Normality:</strong> For any fixed value of X, Y is normally distributed.</p></li>
</ol>
<p>To read more about assumptions and there influence when trainig the linear regression model you can refer to the following link-
<a class="reference external" href="https://towardsdatascience.com/assumptions-of-linear-regression-fdb71ebeaa8b">https://towardsdatascience.com/assumptions-of-linear-regression-fdb71ebeaa8b</a></p>
</section>
<section id="overfitting-and-underfitting">
<h2>Overfitting and Underfitting<a class="headerlink" href="#overfitting-and-underfitting" title="Permalink to this headline">¶</a></h2>
<section id="overfitting">
<h3>Overfitting<a class="headerlink" href="#overfitting" title="Permalink to this headline">¶</a></h3>
<p>Overfitting is a concept in data science and machine learning, which occurs when a statistical model fits exactly against its training data. When this happens, the algorithm unfortunately cannot perform accurately against unseen data, defeating its purpose. Generalization of a model to new data is ultimately what allows us to use machine learning algorithms every day to make predictions and classify data.</p>
<p>When machine learning algorithms are constructed, they leverage a sample dataset to train the model. However, when the model trains for too long on sample data or when the model is too complex, it can start to learn the “noise,” or irrelevant information, within the dataset. When the model memorizes the noise and fits too closely to the training set, the model becomes “overfitted,” and it is unable to generalize well to new data. If a model cannot generalize well to new data, then it will not be able to perform the classification or prediction tasks that it was intended for.</p>
<p>Low error rates and a high variance are good indicators of overfitting. In order to prevent this type of behavior, part of the training dataset is typically set aside as the “test set” to check for overfitting. If the training data has a low error rate and the test data has a high error rate, it signals overfitting.</p>
</section>
<section id="underfitting">
<h3>Underfitting<a class="headerlink" href="#underfitting" title="Permalink to this headline">¶</a></h3>
<p>Underfitting is a scenario in data science and machine learning where a data model is unable to capture the relationship between the input and output variables accurately, generating a high error rate on both the training set and unseen data. It occurs when a model is too simple, which can be a result of a model needing more training time, more input features, or less regularization. Like overfitting, when a model is underfitted, it cannot establish the dominant trend within the data, resulting in training errors and poor performance of the model. If a model cannot generalize well to new data, then it cannot be leveraged for classification or prediction tasks. Generalization of a model to new data is ultimately what allows us to use machine learning algorithms every day to make predictions and classify data.</p>
<p>High bias and low variance are good indicators of underfitting. Since this behavior can be seen while using the training dataset, underfitted models are usually easier to identify than overfitted ones.</p>
<p><img alt="model-over-fitting.webp" src="_images/linear_regression2.png" /></p>
<p>One of the way to identify whether the model is getting overfitted or underfitted is to look at the error as mentioned above in the diagram.</p>
<p><strong>An underfitted model will give high error in both training and testing(validation) data</strong></p>
<p><strong>A overfitted model will give low error in case of training data but comparatively high error on testing(validation) data</strong></p>
<p>There is a connection between these two, as to prevent underfitting you will want your model to be flexible enough to take on irregular distribution of data but that increases the chances of overfitting of the model, thus it is neccessary to stop at an optimal complexity for the model.</p>
<p><img alt="download.png" src="_images/linear_regression3.png" /></p>
</section>
</section>
<section id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Permalink to this headline">¶</a></h2>
<p>You can read more about r2 score from here-</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Coefficient_of_determination">https://en.wikipedia.org/wiki/Coefficient_of_determination</a></p>
<p>and about sklearn coding’s part from here-</p>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html</a></p>
<p><a class="reference external" href="https://towardsdatascience.com/assumptions-of-linear-regression-fdb71ebeaa8b">https://towardsdatascience.com/assumptions-of-linear-regression-fdb71ebeaa8b</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="2.%20KNN.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">K - Nearest Neighbour</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="3.2%20Multi-Variate%20Regression.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Multi Variable Regression</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Coding Blocks Pvt Ltd<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>